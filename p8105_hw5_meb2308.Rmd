---
title: "Homework 5 Solutions"
author: Meghan Bellerose
date: November 7, 2020
output: github_document
---

```{r}
library(tidyverse)
library(broom)
library(ggplot2)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

The data for this problem were compiled by _The Washington Post_ and include information on more than 52,000 criminal homicides that occurred over the past decade in 50 major U.S. cities. 

```{r}
homicides = 
  read_csv("./hom_data/homicide_data.csv")
```

The dataset contains `r nrow(homicides)` rows and `r ncol(homicides)` columns, with each row representing a single homicide. The variables are location of a homicide (latitude and longitude), whether an arrest was made, and demographic information about the victim. 

The following code chunks create a `city_state` variable and provide the proportion of unsolved homicides (number of unresolved homicides / total number of homicides) by city.

```{r}
homicide_df =
  homicides %>% 
  mutate(
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved",
    )
  ) %>% 
  unite("city_state", city:state, sep = ", ", remove = TRUE) %>% 
  select(city_state, resolved) %>% 
  filter(city_state !="Tulsa, AL")
```

```{r}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Try a proportion test for Boston, MA

```{r}
prop.test(
  310, 
  614)
```

Prepare for iteration

```{r}
prop.test(
    aggregate_df %>%  filter(city_state == "Boston, MA") %>%  pull(hom_unsolved),
    aggregate_df %>%  filter(city_state == "Boston, MA") %>%  pull(hom_total)) %>% 
  broom::tidy()

```

Iteration

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust =1))
```
Chicago, Illinois has the largest proportion of unsolved homicides and Richmond, Virginia has the lowest proportion.


# Problem 2

This problem uses data from a longitudinal study with a control and experimental arm and 8 weeks of observation.

First, I'll create a tidy dataframe with the participants' subject IDs, study arms, and observations over time. 

 * I used map to iterate over file names and read in data and saved each result as new variable in a dataframe. 

 * I tidied the result by manipulating file names to include control arm and subject ID then made sure weekly observations were tidy.

```{r, message = FALSE}
long_study_df =
  tibble(
    path = list.files("./data"),
  ) %>% 
  mutate(
    path = str_c("data/", path),
    path_names = path,
    data = map(path, read_csv)) %>% 
  separate(col = path_names, into = c("path_arm", "path_id"), sep = 9) %>% 
  separate(col = path_id, into = c("id", ".csv"), sep = 2) %>% 
  mutate(
    arm = recode(path_arm, "data/con_" = "control", 
    "data/exp_" = "experimental")) %>% 
  select(arm, id, data) %>% 
  unnest(data) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
    )
```


The following code creates a spaghetti plot showing observations on each subject over time.

```{r}
long_study_df %>% 
  ggplot(aes(x = week, y = observation, 
    group = arm, color = arm)) +
  geom_path() + 
  geom_smooth() +
  labs(
    title = "Experimental vs Control Group Obervations over Time",
    x = "Week",
    y = "Observation")
```

This plot shows that the study outcome increased within the experimental over the 8 week period but remained relatively constant within the control group. 


# Problem 3

First I will will set some design elements for later simulations.

* Fix n=30
* Fix σ=5 (variance = σ^2)
* Set μ=0 (mean)

x∼Normal[μ,σ]

This code generates 5000 datasets from the model. For each dataset, I will save μ̂ and the p-value arising from a test of H:μ=0 using α=0.05. I'll repeat for μ={1,2,3,4,5,6}.


```{r}
set.seed(1)

sim_t = function(mu) {

sim_data = 
  tibble(
    true_mu = mu,
    x = rerun(5000, rnorm(n = 30, mean = mu, sd = 5))
  ) %>% 
  mutate(
    t_test = map(.x = x, ~t.test(x = .x, mu = 0)),
    tidy_t = map(.x = t_test, ~broom::tidy(.x))
  ) %>% 
  select(-t_test) %>% 
  unnest(tidy_t) %>% 
  select(true_mu, estimate, p.value)
}

output <- sim_t(mu=0)

for (i in 1:6) {
  temp <- sim_t(i)
  output <- bind_rows(output, temp)
}
```

The following code produces a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. 

```{r message = FALSE}
results_df = 
  output %>% 
  mutate(
    decision = case_when(
       p.value >= 0.05 ~ "fail to reject",
       p.value < 0.05 ~ "reject")
    ) %>% 
  filter(decision == "reject")%>%
  group_by(true_mu) %>%
  summarize(power = n()/5000)
  
results_df %>% 
  ggplot(aes(x = true_mu, y = power)) +
  geom_point(alpha = .5) +
  geom_line(alpha = .5) + 
  labs(
    title = "Proportion of times null hypothesis was rejected",
    x = "True Mean",
    y = "Power"
  )
```

There is a positive correlation between power and the true value of the mean. As the true value of the mean increases, power increases as well. 

The following code produces a plot showing the average estimate of μ̂ on the y axis and the true value of μ on the x axis overlayed with a second plot with the average estimate of μ̂ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 

```{r, message = FALSE}
full_plot_2 =
  output %>% 
  group_by(true_mu) %>% 
  summarise(mean = mean(estimate))
```

```{r, message = FALSE}
reject_plot_3 =
  output %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_mu) %>% 
  summarise(mean = mean(estimate))
```

```{r}
ggplot(full_plot_2, aes(x = true_mu, y = mean)) +
  geom_point() +
  geom_line(color = "green") +
    labs(
    title = "All samples",
    x = "True Mean",
    y = "Mean estimate"
  ) +
ggplot(reject_plot_3, aes(x = true_mu, y = mean)) +
  geom_point() +
  geom_line(color = "magenta") +
  labs(
    title = "Samples with rejected null",
    x = "True Mean",
    y = "Mean estimate"
  )
```

The sample average of the mean across tests for which the null is rejected is not equal to the that for the full sample, although it gets closer as the value of the true mu increases. This occurs because the higher true mu values allow for the detection of a larger sample size, which makes it easier to reject the null hypothesis.


---
title: "Homework 5 Solutions"
author: Meghan Bellerose
date: November 7, 2020
output: github_document
---

```{r, echo = FALSE}
library(tidyverse)
library(broom)
library(ggplot2)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

# Problem 1

The data for this problem were compiled by _The Washington Post_ and include information on more than 52,000 criminal homicides that occurred over the past decade in 50 major U.S. cities. 

```{r}
homicides = 
  read_csv("./hom_data/homicide_data.csv")
```

The dataset contains `r nrow(homicides)` rows and `r ncol(homicides)` columns, with each row representing a single homicide. The variables are location of a homicide (latitude and longitude), whether an arrest was made, and demographic information about the victim. 

The following code chunks create a `city_state` variable and provide the proportion of unsolved homicides (number of unresolved homicides / total number of homicides) by city.

```{r}
homicide_df =
  homicides %>% 
  mutate(
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved",
    )
  ) %>% 
  unite("city_state", city:state, sep = ", ", remove = TRUE) %>% 
  select(city_state, resolved) %>% 
  filter(city_state !="Tulsa, AL")
```

```{r}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Try a proportion test for Boston, MA

```{r}
prop.test(
  310, 
  614)
```

Prepare for iteration

```{r}
prop.test(
    aggregate_df %>%  filter(city_state == "Boston, MA") %>%  pull(hom_unsolved),
    aggregate_df %>%  filter(city_state == "Boston, MA") %>%  pull(hom_total)) %>% 
  broom::tidy()

```

Iteration

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust =1))
```
Chicago, Illinois has the largest proportion of unsolved homicides and Richmond, Virginia has the lowest proportion.


# Probelem 2

This problem uses data from a longitudinal study with a control and experimental arm and 8 weeks of observation.

First, I'll create a tidy dataframe with the participants' subject IDs, study arms, and observations over time. 

 * I used map to iterate over file names and read in data and saved each result as new variable in a dataframe. 

 * I tidied the result by manipulating file names to include control arm and subject ID then made sure weekly observations were tidy.


Prep for iteration:
```{r}
path_df =
  tibble(
    path = list.files("./data"),
  ) %>% 
  mutate(
    path = str_c("data/", path))
  
read.csv(path_df$path[[1]])
```

Iteration:
```{r, error = TRUE}

read_csv = function(x) {
  read_csv = read.csv(x)
}


output = map(path_df, read_csv)

path_df =
  tibble(
    path = list.files("./data"),
  ) %>% 
  mutate(
    path = str_c("data/", path),
    data = map(read_csv, path))

```

```{r, error = TRUE}
plot = 
  path_df %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
    ) %>% 
  ggplot(aes(x = week, y = observation, 
    group = subject_id, color = subject_id)) +
  geom_path() + 
  facet_grid(~control) +
  labs(
    title = "Experimental vs Control Group Obervations over Time",
    x = "Week",
    y = "Observation")
```


The following code creates a spaghetti plot showing observations on each subject over time.



Comment on differences between groups...



# Problem 3

First I will will set some design elements for later simulations.

* Fix n=30
* Fix σ=5 (variance = σ^2)
* Set μ=0 (mean)

x∼Normal[μ,σ]

This code generates 5000 datasets from the model.

```{r}
simulation = function(n = 30, beta0 = 1, beta1) {

sim_data = 
  tibble(
    x = rnorm(n, mean = 1, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, 25)
  )

tt <- t.test(y ~ x, sim_data)
tidy(tt)

linear_model = lm(y ~ x, data = sim_data) %>% 
  broom::tidy()

linear_model
}
```

For each dataset, save μ̂ and the p-value arising from a test of H:μ=0 using α=0.05

First dataset:
```{r}
sim_results = 
  rerun(5000, simulation(30, 5, 0)) %>% 
  bind_rows() %>% 
  select(term, estimate, p.value) %>% 
  knitr::kable()
```

Iterate over all 5000 datasets:

```{r}
sim_results = 
  tibble(beta1 = c(0,1,2,3,4,5,6)) %>% 
  mutate(
    sim_lists = map(.x = beta1, ~rerun(5000, simulation(beta1 = .x))),
    sim_estimates = map(sim_lists, bind_rows)
    ) %>%     
  select(~sim_estimates) %>% 
  unnest(sim_estimates)
```


To obtain the estimate and p-value, use broom::tidy to clean the output of t.test.

Repeat the above for μ={1,2,3,4,5,6}, and complete the following:

* Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis. Describe the association between effect size and power.


* Make a plot showing the average estimate of μ̂ on the y axis and the true value of μ on the x axis. Make a second plot (or overlay on the first) the average estimate of μ̂ only in samples for which the null was rejected on the y axis and the true value of μ on the x axis. 

(filter to have less than 0.5 for p value)

Is the sample average of μ̂ across tests for which the null is rejected approximately equal to the true value of μ ? Why or why not?


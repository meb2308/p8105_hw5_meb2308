Homework 5 Solutions
================
Meghan Bellerose
November 7, 2020

    ## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.0 â”€â”€

    ## âœ“ ggplot2 3.3.2     âœ“ purrr   0.3.4
    ## âœ“ tibble  3.0.4     âœ“ dplyr   1.0.2
    ## âœ“ tidyr   1.1.2     âœ“ stringr 1.4.0
    ## âœ“ readr   1.3.1     âœ“ forcats 0.5.0

    ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

# Problem 1

The data for this problem were compiled by *The Washington Post* and
include information on more than 52,000 criminal homicides that occurred
over the past decade in 50 major U.S. cities.

``` r
homicides = 
  read_csv("./hom_data/homicide_data.csv")
```

    ## Parsed with column specification:
    ## cols(
    ##   uid = col_character(),
    ##   reported_date = col_double(),
    ##   victim_last = col_character(),
    ##   victim_first = col_character(),
    ##   victim_race = col_character(),
    ##   victim_age = col_character(),
    ##   victim_sex = col_character(),
    ##   city = col_character(),
    ##   state = col_character(),
    ##   lat = col_double(),
    ##   lon = col_double(),
    ##   disposition = col_character()
    ## )

The dataset contains 52179 rows and 12 columns, with each row
representing a single homicide. The variables are location of a homicide
(latitude and longitude), whether an arrest was made, and demographic
information about the victim.

The following code chunks create a `city_state` variable and provide the
proportion of unsolved homicides (number of unresolved homicides / total
number of homicides) by city.

``` r
homicide_df =
  homicides %>% 
  mutate(
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest" ~ "unsolved",
      disposition == "Closed by arrest" ~ "solved",
    )
  ) %>% 
  unite("city_state", city:state, sep = ", ", remove = TRUE) %>% 
  select(city_state, resolved) %>% 
  filter(city_state !="Tulsa, AL")
```

``` r
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

    ## `summarise()` ungrouping output (override with `.groups` argument)

Try a proportion test for Boston, MA

``` r
prop.test(
  310, 
  614)
```

    ## 
    ##  1-sample proportions test with continuity correction
    ## 
    ## data:  310 out of 614, null probability 0.5
    ## X-squared = 0.040717, df = 1, p-value = 0.8401
    ## alternative hypothesis: true p is not equal to 0.5
    ## 95 percent confidence interval:
    ##  0.4646219 0.5450881
    ## sample estimates:
    ##        p 
    ## 0.504886

Prepare for iteration

``` r
prop.test(
    aggregate_df %>%  filter(city_state == "Boston, MA") %>%  pull(hom_unsolved),
    aggregate_df %>%  filter(city_state == "Boston, MA") %>%  pull(hom_total)) %>% 
  broom::tidy()
```

    ## # A tibble: 1 x 8
    ##   estimate statistic p.value parameter conf.low conf.high method     alternative
    ##      <dbl>     <dbl>   <dbl>     <int>    <dbl>     <dbl> <chr>      <chr>      
    ## 1    0.505    0.0407   0.840         1    0.465     0.545 1-sample â€¦ two.sided

Iteration

``` r
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```

``` r
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust =1))
```

<img src="p8105_hw5_meb2308_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />
Chicago, Illinois has the largest proportion of unsolved homicides and
Richmond, Virginia has the lowest proportion.

# Probelem 2

This problem uses data from a longitudinal study with a control and
experimental arm and 8 weeks of observation.

First, Iâ€™ll create a tidy dataframe with the participantsâ€™ subject IDs,
study arms, and observations over time.

  - I used map to iterate over file names and read in data and saved
    each result as new variable in a dataframe.

  - I tidied the result by manipulating file names to include control
    arm and subject ID then made sure weekly observations were tidy.

Prep for iteration:

``` r
path_df =
  tibble(
    path = list.files("./data"),
  ) %>% 
  mutate(
    path = str_c("data/", path))
  
read.csv(path_df$path[[1]])
```

    ##   week_1 week_2 week_3 week_4 week_5 week_6 week_7 week_8
    ## 1    0.2  -1.31   0.66   1.96   0.23   1.09   0.05   1.94

Iteration:

``` r
read_csv = function(x) {
  read_csv = read.csv(x)
}


output = map(path_df, read_csv)
```

    ## Error in file(file, "rt"): invalid 'description' argument

``` r
path_df =
  tibble(
    path = list.files("./data"),
  ) %>% 
  mutate(
    path = str_c("data/", path),
    data = map(read_csv, path))
```

    ## Error: Problem with `mutate()` input `data`.
    ## x `.x` must be a vector, not a function
    ## â„¹ Input `data` is `map(read_csv, path)`.

``` r
plot = 
  path_df %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
    ) %>% 
  ggplot(aes(x = week, y = observation, 
    group = subject_id, color = subject_id)) +
  geom_path() + 
  facet_grid(~control) +
  labs(
    title = "Experimental vs Control Group Obervations over Time",
    x = "Week",
    y = "Observation")
```

    ## Error: Can't subset columns that don't exist.
    ## [31mx[39m Column `week_1` doesn't exist.

The following code creates a spaghetti plot showing observations on each
subject over time.

Comment on differences between groupsâ€¦

# Problem 3

First I will will set some design elements for later simulations.

  - Fix n=30
  - Fix Ïƒ=5 (variance = Ïƒ^2)
  - Set Î¼=0 (mean)

xâˆ¼Normal\[Î¼,Ïƒ\]

This code generates 5000 datasets from the model.

``` r
simulation = function(n = 30, beta0 = 1, beta1) {

sim_data = 
  tibble(
    x = rnorm(n, mean = 1, sd = 1),
    y = beta0 + beta1 * x + rnorm(n, 0, 25)
  )

linear_model = lm(y ~ x, data = sim_data) %>% 
  broom::tidy()

linear_model
}
```

For each dataset, save Î¼Ì‚ and the p-value arising from a test of H:Î¼=0
using Î±=0.05

First dataset:

``` r
sim_results = 
  rerun(5000, simulation(30, 5, 0)) %>% 
  bind_rows() %>% 
  select(term, estimate, p.value) %>% 
  knitr::kable()
```

Iterate over all 5000 datasets:

To obtain the estimate and p-value, use broom::tidy to clean the output
of t.test.

Repeat the above for Î¼={1,2,3,4,5,6}, and complete the following:

  - Make a plot showing the proportion of times the null was rejected
    (the power of the test) on the y axis and the true value of Î¼ on the
    x axis. Describe the association between effect size and power.

  - Make a plot showing the average estimate of Î¼Ì‚ on the y axis and the
    true value of Î¼ on the x axis. Make a second plot (or overlay on the
    first) the average estimate of Î¼Ì‚ only in samples for which the null
    was rejected on the y axis and the true value of Î¼ on the x axis.

(filter to have less than 0.5 for p value)

Is the sample average of Î¼Ì‚ across tests for which the null is rejected
approximately equal to the true value of Î¼ ? Why or why not?
